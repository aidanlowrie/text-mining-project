{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "030da081",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The first thing we need to do is preprocess our data. Let's break this down into two smaller steps:\n",
    "\n",
    "1. **Download** the data into a **text file**.\n",
    "2. **Process** the text file data into a list of **weighted surface co-occurences**.\n",
    "\n",
    "Below, we have implemented these steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4536cbac",
   "metadata": {},
   "source": [
    "## 1. Downloading the Data\n",
    "### E-Utilities and Biopython \n",
    "\n",
    "To download the data we need, we will make use of E-Utilities (NCBI Entrez Programming Utilities), a set of tools designed to facilitate the process of downloading large sets of bioinformatics data.\n",
    "\n",
    "A general introduction to the E-Utilities:\n",
    "- https://www.ncbi.nlm.nih.gov/books/NBK25497/\n",
    "- *'A set of nine server-side programs that provide a stable interface into the Entrez query and database system at the NCBI'*.\n",
    "- Uses a fixed URL syntax that translates a standard set of input parameters into the values necessary for various NCBI software components to search for and retrieve the requested data.\n",
    "- The E-utilities are therefore the structured interface to the Entrez system, which currently includes 38 databases covering a variety of biomedical data, including nucleotide and protein sequences, gene records, three-dimensional molecular structures, and the biomedical literature.\n",
    "- To access data, a piece of software posts an E-utility URL to NCBI, then retrieves the results of this and processes the data.\n",
    "- It can use any computer languages that can send a URL to the E-utilities server and interpret the XML response (i.e. Python, Perl, Java, C++).\n",
    "- NCBI requests that users limit requests to no more than 3 per second.\n",
    "\n",
    "From this, I have gleaned that I can use a combination of **ESearch** and **EFetch** to find and retrieve the data I want.\n",
    "\n",
    "In order to make use of the tools, a program must post an 'E-Utility URL' to NCBI. **BioPython** is a library that provides a tool called Entrez to send these URLs using Python. \n",
    "\n",
    "Below, it is used to fetch all articles related to the term 'biology'. It then saves the data to one text file, 'bio_corpus.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbacb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "Entrez.email = 'aidanlowrie@example.com'\n",
    "search_word = 'biology'\n",
    "\n",
    "search_handle = Entrez.esearch(db='pubmed', term=search_word, retmax=100000)\n",
    "record = Entrez.read(search_handle)\n",
    "search_handle.close()\n",
    "\n",
    "uids = record['IdList']\n",
    "\n",
    "fetch_handle = Entrez.efetch(db=\"pubmed\", id=','.join(uids), \n",
    "                       rettype=\"abstract\", retmode=\"xml\") # Return data in XML form.\n",
    "\n",
    "data = fetch_handle.read()\n",
    "fetch_handle.close()\n",
    "\n",
    "# Now parse the XML\n",
    "root = ET.fromstring(data)\n",
    "\n",
    "# The path to the abstract text will depend on the structure of the returned XML, \n",
    "# but it will be something like this:\n",
    "abstracts = root.findall(\".//AbstractText\")\n",
    "\n",
    "# Write just the abstract text to the file\n",
    "with open(\"data/bio_corpus.txt\", \"w\") as file:\n",
    "    for abstract in abstracts:\n",
    "        file.write(str(abstract.text) + \"\\n\\n\")  # Add two newlines for separation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dd07c9a",
   "metadata": {},
   "source": [
    "According to the [PubMed help document](https://pubmed.ncbi.nlm.nih.gov/help/#understanding-docsum), searches of its database return results sorted by a Best Match algorithm. This algorithm puts a weight on each result based on its relevance to the search query, and orders results according to this weight. Recently-published and highly-cited articles are given a higher weight by this algorithm. More details can be found [here](https://pubmed.ncbi.nlm.nih.gov/help/#understanding-docsum).\n",
    "\n",
    "What this means for the data is that it will reflect the **current state of biology research in April 2023**.\n",
    "\n",
    "Originally the goal was to retrieve the full text for each article, but this is not allowed by the database. Therefore, abstracts have been downloaded instead. The resulting file's size is around 27mb - a similar size to the reuters corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "179f42b7",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Preprocessing the Text File\n",
    "### The CorpusProcessor\n",
    "\n",
    "In assignment 2, I created a ```CorpusProcessor``` class. An object of this class takes in any corpus of text and processes it by running it through a pipeline of (customisable) steps, outlined below:\n",
    "1. Breaking the corpus into sentences.\n",
    "2. Tagging and lemmatizing the corpus.\n",
    "3. Removing words with fewer than three alphanumeric characters.\n",
    "4. Removing stopwords.\n",
    "5. Removing infrequent words.\n",
    "6. Finding surface co_occurrences.\n",
    "7. Removing the least frequent co_occurrences.\n",
    "\n",
    "With the data processed into a text file, it can now be passed into a CorpusProcessor object, which will automatically process the text to our specifications. \n",
    "\n",
    "To pass it into the object, the data must first be processed into a class with a ```sents()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ffc196",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Importing nltk resources.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m \u001b[39mimport\u001b[39;00m PlaintextCorpusReader\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m sent_tokenize, word_tokenize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Importing nltk resources.\n",
    "import nltk\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Making a CorpusReader class that I can pass into my CorpusProcessor\n",
    "class CorpusReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.raw_text = ''\n",
    "        self.sentences = self.read_to_sentences()\n",
    "        \n",
    "    def sents(self):\n",
    "        return [sentence for sentence in self.sentences]\n",
    "    \n",
    "    # This function opens a file and tokenizes its contents, returning a series of sentences.\n",
    "    def read_to_sentences(self):\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            self.raw_text = file.read()\n",
    "        tokenized_sentences = []\n",
    "        sentences = sent_tokenize(self.raw_text)\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentences.append(word_tokenize(sentence))\n",
    "        return tokenized_sentences\n",
    "\n",
    "corpus = CorpusReader('data/bio_corpus.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44a23af7",
   "metadata": {},
   "source": [
    "Now, the object can be passed in to the CorpusProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044d1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the CorpusProcessor class.\n",
    "from surface_cooccurrences import CorpusProcessor\n",
    "\n",
    "# Passing the CorpusReader object into the CorpusProcessor.\n",
    "processed_corpus = CorpusProcessor(corpus, \n",
    "                                   remove_most_frequent=20, # Remove 20 most frequent words.\n",
    "                                   frequency_threshold=25, # Remove words that appear under 25 times throughout the corpus. \n",
    "                                   sc_frequency_threshold=10) # Remove surface co-occurrence word pairs that appear together under 10 times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c25a2b2c",
   "metadata": {},
   "source": [
    "From this, a list of **noun surface co-occurrences** is generated. This is what we will be focusing on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c40dfb24",
   "metadata": {},
   "source": [
    "### Weighted Surface Co-occurrences \n",
    "\n",
    "With our list of surface-cooccurrences, smoothed_ppmi should be carried out in order to produce weighted surface co-occurrences. This has been implemented below. The processed surface-cooccurrences is then saved to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b695c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Carry out sppmi function. \n",
    "def smoothed_ppmi(o_11, r_1, c_1, n, alpha=0.75):\n",
    "    result = ppmi(o_11, r_1, c_1, n, alpha = 0.75)\n",
    "    return result\n",
    "\n",
    "# Carry out ppmi function. \n",
    "def ppmi(o_11, r_1, c_1, n, alpha=0):\n",
    "    if alpha > 0:\n",
    "        c_1 = c_1 ** alpha    \n",
    "    observed = o_11\n",
    "    expected = (r_1*c_1)/n \n",
    "    result = math.log(observed/max(0.001, expected), 2)\n",
    "    return max(0, result)\n",
    "\n",
    "# Function takes a CorpusProcessor object and carries out a function to produce weighted surface co-occurrences. \n",
    "def weighted_surface_cooccurrences(corpus_processor_object, measure_function):\n",
    "    adjusted_surface_frequencies = Counter()\n",
    "    for key, value in corpus_processor_object.surface_cooccurrences.items():\n",
    "        o_11 = value\n",
    "        r_1 = corpus_processor_object.filtered_lemma_frequencies[key[0]]\n",
    "        c_1 = corpus_processor_object.filtered_lemma_frequencies[key[1]]\n",
    "        n = sum(corpus_processor_object.surface_cooccurrences.values())\n",
    "        adjusted_surface_frequencies[key] = measure_function(o_11, r_1, c_1, n)\n",
    "    return adjusted_surface_frequencies\n",
    "\n",
    "# Carrying out sppmi on our weighted surface cooccurrences data in order to produce weighted cooccurrences.\n",
    "sc_sppmi = weighted_surface_cooccurrences(processed_corpus, smoothed_ppmi)\n",
    "processed_corpus = sc_sppmi\n",
    "pair_frequencies = [(key, value) for key, value in processed_corpus.items()]\n",
    "sorted_pairs = sorted(pair_frequencies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Saving weighted surface cooccurrence data to a csv.\n",
    "with open('data/bio_surface_cooccurrences.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Word1', 'Word2', 'Frequency'])\n",
    "    for pair in sorted_pairs:\n",
    "        csv_writer.writerow([pair[0][0].split('-')[0], pair[0][1].split('-')[0], pair[1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3caa5bfc",
   "metadata": {},
   "source": [
    "## Saving processed corpus to a text file.\n",
    "\n",
    "In our next section, key-word extraction, we will need access to the entire processed corpus. This will be used to get the semantic data necessary for keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c546fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_corpus.txt', w) as file:\n",
    "    for sentence in processed_corpus.corpus:\n",
    "        file.write(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70dfdbc5",
   "metadata": {},
   "source": [
    "# New Extra Stuff For Relationship Extraction\n",
    "\n",
    "I haven't had time to get this into the notebook in a neat way, but I thought you guys might appreciate having access to all my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0dbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing nltk resources.\n",
    "import nltk\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "\n",
    "# Opening corpus.\n",
    "with open(\"data/genetics_corpus.txt\", \"r\") as file:\n",
    "    corpus_text = file.read()\n",
    "\n",
    "corpus_sentences = sent_tokenize(corpus_text)\n",
    "\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "with open(\"triplets.csv\", \"w\") as file:\n",
    "    field_names = ['head', 'type', 'tail']\n",
    "    writer = csv.DictWriter(file, fieldnames=field_names)    \n",
    "    for sentence in corpus_sentences:\n",
    "        extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "        triplets = extract_triplets(extracted_text[0])\n",
    "        for triplet in triplets:\n",
    "            writer.writerow(triplet)\n",
    "        print([(triplet['head'], triplet['type'], triplet['tail']) for triplet in triplets])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

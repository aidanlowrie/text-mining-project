{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "The aim of this notebook is to outline the steps taken to produce the data - a series of relevant **subject, relationship, object** triplets - that will be visualised in a network in future steps. The process has been broken down as follows:\n",
    "\n",
    "1. Downloading the Data with E-Utilities and Biopython.\n",
    "2. Relationship Extraction using REBEL.\n",
    "3. Semantic Matching using BioBERT.\n",
    "4. Key-Word Extraction using KeyBERT and BioBERT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading the Data with E-Utilities and Biopython.\n",
    "To download the data, we will make use of E-Utilities (NCBI Entrez Programming Utilities), a set of tools designed to facilitate the process of downloading large sets of bioinformatic data.\n",
    "\n",
    "A general introduction to the E-Utilities:\n",
    "- https://www.ncbi.nlm.nih.gov/books/NBK25497/\n",
    "- *'A set of nine server-side programs that provide a stable interface into the Entrez query and database system at the NCBI'*.\n",
    "- Uses a fixed URL syntax that translates a standard set of input parameters into the values necessary for various NCBI software components to search for and retrieve the requested data.\n",
    "- The E-utilities are therefore the structured interface to the Entrez system, which currently includes 38 databases covering a variety of biomedical data, including nucleotide and protein sequences, gene records, three-dimensional molecular structures, and the biomedical literature.\n",
    "- To access data, a piece of software posts an E-utility URL to NCBI, then retrieves the results of this and processes the data.\n",
    "- It can use any computer languages that can send a URL to the E-utilities server and interpret the XML response (i.e. Python, Perl, Java, C++).\n",
    "- NCBI requests that users limit requests to no more than 3 per second.\n",
    "\n",
    "A combination of **ESearch** and **EFetch** can be used to find and retrieve the relevant data.\n",
    "\n",
    "In order to make use of the tools, a program must post an 'E-Utility URL' to NCBI. **BioPython** is a library that provides a tool called Entrez to send these URLs using Python. \n",
    "\n",
    "Below, it is used to **fetch** all article **abstracts** related to the term **'biology'**. It then saves the data to one text file, 'bio_corpus.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Setting email.\n",
    "Entrez.email = 'aidanlowrie@example.com'\n",
    "\n",
    "# Setting search word.\n",
    "search_word = 'biology'\n",
    "\n",
    "# Search for PubMed articles related to the search word 'biology', returning up to 100,000 results. \n",
    "search_handle = Entrez.esearch(db='pubmed', term=search_word, retmax=100000)\n",
    "record = Entrez.read(search_handle)\n",
    "search_handle.close()\n",
    "\n",
    "# A list of uids is necessary for fetching the actual abstracts.\n",
    "uids = record['IdList']\n",
    "\n",
    "# Fetch the abstracts in XML form, so that the actual abstract may be extracted.\n",
    "fetch_handle = Entrez.efetch(db=\"pubmed\", id=','.join(uids), \n",
    "                             rettype=\"abstract\", retmode=\"xml\") # Return data in XML form.\n",
    "data = fetch_handle.read()\n",
    "fetch_handle.close()\n",
    "\n",
    "# Parsing the XML.\n",
    "root = ET.fromstring(data)\n",
    "# Extracting the abstracts themselves from the returned data.\n",
    "abstracts = root.findall(\".//AbstractText\")\n",
    "\n",
    "# Write the abstracts to a file.\n",
    "with open(\"data/bio_corpus.txt\", \"w\") as file:\n",
    "    for abstract in abstracts:\n",
    "        file.write(str(abstract.text) + \"\\n\\n\") # Two newlines are added between abstracts, for clarity.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Relationship Extraction with REBEL \n",
    "Relationship extraction involves finding **triplets** - **subject (head)**, **relationship (type)** and **object (tail)** - in a corpus. While this can be achieved through a variety of machine learning techniques including pattern matching and supervised machine learning, we have chosen to use REBEL (Relationship Extraction By End-to-end Language generation).\n",
    "\n",
    "[REBEL](https://aclanthology.org/2021.findings-emnlp.204.pdf) is an open source relationship extraction seq2seq model released in 2021. We have chosen to use it due to its **state-of-the-art performance** and **ease-of-use**.\n",
    "\n",
    "Before being passed into the model, we used **NLTK** to **break the corpus into sentences**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing nltk resources.\n",
    "import nltk\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Opening corpus.\n",
    "with open(\"data/bio_corpus.txt\", \"r\") as file:\n",
    "    corpus_text = file.read()\n",
    "\n",
    "corpus_sentences = sent_tokenize(corpus_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ```extract_triplets``` Function.\n",
    "This function was lifted directly from the [huggingface REBEL docs](https://huggingface.co/Babelscape/rebel-large). It is designed to **parse the text** generated by REBEL into a **list of triplets**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse REBEL output into a list of triplets. \n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below **loads the REBEL model** and **writes** the **extracted triplets to a file**. \n",
    "\n",
    "*The process had to be carried out in batches over several nights, which is why the code was adapted to include a 'start line'.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from transformers import pipeline\n",
    "\n",
    "# Loading the model.\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "# Establishing a start line.\n",
    "start_line = 15000\n",
    "\n",
    "# The model can only handle tokens of max length 1024 tokens. Those exceeding this capacity aren't considered in the dataset. (This is very rare, but the check is necessary.)\n",
    "max_token_length = 1024\n",
    "\n",
    "# Opening a csv file for triplet storage.\n",
    "with open(\"data/triplets_batch4.csv\", \"w\") as file:\n",
    "    # Field names.\n",
    "    field_names = ['head', 'type', 'tail']\n",
    "    writer = csv.DictWriter(file, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    for i, sentence in enumerate(corpus_sentences):\n",
    "        if i > start_line and len(triplet_extractor.tokenizer.encode(sentence)) <= max_token_length:\n",
    "            extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "            triplets = extract_triplets(extracted_text[0])\n",
    "            for triplet in triplets:\n",
    "                writer.writerow(triplet)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The triplet batches are then merged into a single file and duplicates are removed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def csv_union(csv_path_list, output_csv_path):\n",
    "    dfs = []\n",
    "    for csv in csv_path_list:\n",
    "        dfs.append(pd.read_csv(csv))\n",
    "    df_union = pd.concat(dfs).drop_duplicates()\n",
    "    df_union.to_csv(output_csv_path, index=False)\n",
    "\n",
    "csv_union(csv_path_list=['data/triplets_batch1.csv', 'data/triplets_batch2.csv', 'data/triplets_batch3.csv', 'data/triplets_batch4.csv'], output_csv_path='data/triplets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keyword Extraction with KeyBERT and BioBERT\n",
    "\n",
    "https://towardsdatascience.com/how-to-extract-relevant-keywords-with-keybert-6e7b3cf889ae (```KeyBert``` useful info.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading corpus.\n",
    "with open('data/bio_corpus.txt', 'r') as file:\n",
    "    abstracts = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from keybert import KeyBERT\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Loading the BioBERT model.\n",
    "model_name = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "biobert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Loading the KeyBERT model running on BioBERT \n",
    "kw_model = KeyBERT(model=biobert_model)\n",
    "\n",
    "# Loading triplets dataframe.\n",
    "df = pd.read_csv('data/triplets.csv', names=['head', 'type', 'tail'])\n",
    "# Remove null values.\n",
    "df = df[df['head'].notna() & df['tail'].notna()]\n",
    "\n",
    "# Creating a list of stopwords from NLTK.\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "keywords = set()\n",
    "with open(\"data/keywords.csv\", \"w\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Keyword'])\n",
    "    writer.writeheader()   \n",
    "    for abstract in abstracts:\n",
    "        # Extracting unigrams.\n",
    "        new_keyword_list = set([keyword for keyword, _ in kw_model.extract_keywords(abstract, keyphrase_ngram_range=(1, 1), stop_words=stop_words)])\n",
    "        novel_keywords = new_keyword_list - keywords\n",
    "        keywords.update(novel_keywords)\n",
    "        for novel_keyword in novel_keywords:\n",
    "            writer.writerow({'Keyword': novel_keyword})\n",
    "        # Extracting bigrams.\n",
    "        new_keyword_list = set([keyword for keyword, _ in kw_model.extract_keywords(abstract, keyphrase_ngram_range=(2, 2), stop_words=stop_words)])\n",
    "        novel_keywords = new_keyword_list - keywords\n",
    "        keywords.update(novel_keywords)\n",
    "        for novel_keyword in novel_keywords:\n",
    "            writer.writerow({'Keyword': novel_keyword})\n",
    "        # Extracting trigrams.\n",
    "        new_keyword_list = set([keyword for keyword, _ in kw_model.extract_keywords(abstract, keyphrase_ngram_range=(3, 3), stop_words=stop_words)])\n",
    "        novel_keywords = new_keyword_list - keywords\n",
    "        keywords.update(novel_keywords)\n",
    "        for novel_keyword in novel_keywords:\n",
    "            writer.writerow({'Keyword': novel_keyword})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/tvn5hxb174732ns7m01vcfgr0000gn/T/ipykernel_5246/803617393.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].str.replace('-', ' ')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_dataframe(df, relevant_column_names, filter_set):\n",
    "    for column_name in relevant_column_names:\n",
    "        # Split hyphenated words.\n",
    "        df[column_name] = df[column_name].str.replace('-', ' ')\n",
    "        # Remove words that are in the filter set.\n",
    "        df = df[~df[column_name].isin(filter_set)]\n",
    "    return df\n",
    "\n",
    "keyword_data = pd.read_csv('data/keywords.csv')\n",
    "keywords = keyword_data['Keyword'].tolist()\n",
    "filtered_df = filter_dataframe(df=df, relevant_column_names=['head', 'tail'], filter_set=keywords)\n",
    "filtered_df.to_csv('data/filtered_triplets.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Matching using BioBERT\n",
    "\n",
    "In the previous steps, triplets were extracted from the corpus using REBEL and filtered depending on whether they were flagged as key terms by **KeyBERT**. But many extracted terms are semantically near-identical. In such cases, they should be **merged** so that only one word is considered. This is called **Semantic Matching**. In order to carry out this process, we will use **BioBERT**.\n",
    "\n",
    "BioBERT is a BERT model **pre-trained on PubMed articles** as well as other biological content. It is used as follows in the code:\n",
    "1. **Semantic data** for each 'head' and 'tail' entry are obtained from BioBERT embeddings. \n",
    "2. **Cosine similarity** scores are then used to compare terms, merging them if they meet a certain similarity threshold by **replacing every instance of one term with the other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of each unique term in the df.\n",
    "unique_words = list(set(df[\"head\"].unique().tolist() \n",
    "                        + df[\"tail\"].unique().tolist()))\n",
    "\n",
    "# Tokenise.\n",
    "unique_words_tokenised = [tokenizer(word, return_tensors=\"pt\") for word in unique_words]\n",
    "\n",
    "# Feeding the tokenised words into the model to get a list of unique_word_embeddings.\n",
    "with torch.no_grad():\n",
    "    unique_word_outputs = [biobert_model(**tokens) for tokens in unique_words_tokenised]\n",
    "unique_word_embeddings = [output.last_hidden_state.mean(dim=1).numpy() for output in unique_word_outputs]\n",
    "\n",
    "# Create a DataFrame with the words and their embeddings\n",
    "df_embeddings = pd.DataFrame({\n",
    "    'word': unique_words,\n",
    "    'embedding': unique_word_embeddings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS 1 Coding variants 2 coding variants cos 0.91758 lev 0.9333333333333333\n",
      "SUCCESS 1 socioeconomic factors 2 socioeconomic indicators cos 0.9773717 lev 0.7916666666666666\n",
      "SUCCESS 1 postmenopausal 2 post-menopausal cos 0.98054916 lev 0.9333333333333333\n",
      "SUCCESS 1 Parkinsonism 2 parkinsonism cos 0.8776088 lev 0.9166666666666666\n",
      "SUCCESS 1 plaque 2 plaques cos 0.8970699 lev 0.8571428571428572\n",
      "SUCCESS 1 microglial 2 microglia cos 0.9648881 lev 0.9\n",
      "SUCCESS 1 diplonemid 2 diplonemids cos 0.99051666 lev 0.9090909090909091\n",
      "SUCCESS 1 university hospital 2 University Hospital cos 0.8423207 lev 0.8947368421052632\n",
      "SUCCESS 1 quality-of-life 2 quality of life cos 0.9663539 lev 0.8666666666666667\n",
      "SUCCESS 1 COL8A1 2 COL11A1 cos 0.97448915 lev 0.7142857142857143\n",
      "SUCCESS 1 COL8A1 2 COL6A3 cos 0.9819741 lev 0.6666666666666667\n",
      "SUCCESS 1 COL8A1 2 COL10A1 cos 0.98273283 lev 0.7142857142857143\n",
      "SUCCESS 1 intermediate state 2 intermediate stage cos 0.8887357 lev 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "def get_cosine_similarity(df_embeddings, word_1, word_2):\n",
    "    # Get the embeddings for the two words\n",
    "    embedding1_df = df_embeddings[df_embeddings['word'] == word_1]['embedding']\n",
    "    embedding2_df = df_embeddings[df_embeddings['word'] == word_2]['embedding']\n",
    "    if embedding1_df.empty or embedding2_df.empty:\n",
    "        return 0\n",
    "    else:\n",
    "        embedding1 = embedding1_df.values[0]\n",
    "        embedding2 = embedding2_df.values[0]\n",
    "        # Compute and return the cosine similarity\n",
    "        return cosine_similarity(embedding1, embedding2)[0][0] # type: ignore\n",
    "\n",
    "\n",
    "def get_levenshtein_similarity(word_1, word_2):\n",
    "    return 1 - Levenshtein.distance(word_1, word_2) / max(len(word_1), len(word_2))\n",
    "\n",
    "# Merge similar words by rippling through the keyword list and comparing against others, then removing from list.\n",
    "def merge_similar_words(df, df_embeddings, cosine_threshold, levenshtein_threshold):\n",
    "    word_replacements = {}\n",
    "    unique_words = df_embeddings['word'].to_list()\n",
    "    for word_1 in unique_words:\n",
    "        unique_words.remove(word_1)\n",
    "        for word_2 in unique_words:\n",
    "            levenshtein_score = get_levenshtein_similarity(word_1=word_1, word_2=word_2)\n",
    "            cosine_score = get_cosine_similarity(df_embeddings=df_embeddings, word_1=word_1, word_2=word_2)\n",
    "            # print('FAILURE', '1', word_1, '2', word_2, 'cos', cosine_score, 'lev', levenshtein_score)\n",
    "            if  (cosine_score > cosine_threshold and levenshtein_score > levenshtein_threshold) or levenshtein_score > 0.85:\n",
    "                print('SUCCESS', '1', word_1, '2', word_2, 'cos', cosine_score, 'lev', levenshtein_score)\n",
    "                if word_1.lower() == word_1 or word_2.lower() == word_2:\n",
    "                    word_1 = word_1.lower()\n",
    "                    word_2 = word_2.lower()\n",
    "                if len(word_1) < len(word_2):\n",
    "                    winner = word_1\n",
    "                elif len(word_2) < len(word_1):\n",
    "                    winner = word_2\n",
    "                else:\n",
    "                    winner = random.choice((word_1, word_2))\n",
    "                if winner == word_1:\n",
    "                    loser = word_2\n",
    "                else:\n",
    "                    loser = word_1\n",
    "                word_replacements[loser] = winner\n",
    "    df_replaced = df.replace(word_replacements)\n",
    "    return df_replaced\n",
    "\n",
    "df_replaced = merge_similar_words(df=df, df_embeddings=df_embeddings, cosine_threshold=0.97, levenshtein_threshold=0.6)\n",
    "df_replaced.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

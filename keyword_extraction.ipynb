{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc874a8a",
   "metadata": {},
   "source": [
    "# Key Word Extraction\n",
    "\n",
    "## BioBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c35a9",
   "metadata": {},
   "source": [
    "The list of surface cooccurrences generated is great. But let's look at the top 25 most frequent word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f0cd1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sorted_pairs = []\n",
    "with open('data/genetics_surface_cooccurrences.csv', 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    # csv_reader read first row\n",
    "    for row in csv_reader:\n",
    "        word1, word2, frequency = row\n",
    "        sorted_pairs.append([word1, word2, frequency])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3f53d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Word1', 'Word2', 'Frequency'], ['turku', 'turku', '17.314512438893168'], ['lille', 'lille', '17.075681452588835'], ['aacute', 'aacute', '17.01017840402054'], ['louisville', 'louisville', '16.999207286199535'], ['gerais', 'minas', '16.993550442336325'], ['aires', 'buenos', '16.98815209769054'], ['buenos', 'aires', '16.977011540085343'], ['cape', 'town', '16.90829879000133'], ['town', 'cape', '16.90829879000133'], ['compostela', 'santiago', '16.88464752193668'], ['aviv', 'tel', '16.861132430899236'], ['tel', 'aviv', '16.851003425126496'], ['aacute', 'eacute', '16.848427334023928'], ['eacute', 'aacute', '16.83614900424549'], ['freiburg', 'freiburg', '16.834750714281245'], ['maastricht', 'maastricht', '16.833279747579173'], ['minas', 'gerais', '16.78324995598144'], ['nashville', 'vanderbilt', '16.762759919764168'], ['bern', 'bern', '16.741142450265244'], ['hiroshima', 'hiroshima', '16.720671786825555'], ['humboldt', 'berlin', '16.720671786825555'], ['preto', 'ribeir√£o', '16.707251271063235'], ['vanderbilt', 'nashville', '16.662704375824443'], ['santiago', 'compostela', '16.66220346159512']]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_pairs[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac790a0",
   "metadata": {},
   "source": [
    "As you can see, most of these cooccurrences aren't related to biology. The next step will involve filtering out word pairs that aren't deemed to be related to biology. This will be achieved by using **BioBERT**, a pre-trained BERT model.\n",
    "\n",
    "To do this, I will carry out the following steps.\n",
    "- Load my surface cooccurrences data into a Pandas Dataframe.\n",
    "- Load a BERT model.\n",
    "- Use the BERT model to generate a list of key words. The BERT model will look at words and measure their similarity to the word 'biology'. If they are under a certain similarity threshold, they will be deleted from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0c2ac31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>turku</td>\n",
       "      <td>turku</td>\n",
       "      <td>17.314512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lille</td>\n",
       "      <td>lille</td>\n",
       "      <td>17.075681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aacute</td>\n",
       "      <td>aacute</td>\n",
       "      <td>17.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>louisville</td>\n",
       "      <td>louisville</td>\n",
       "      <td>16.999207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gerais</td>\n",
       "      <td>minas</td>\n",
       "      <td>16.993550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word1       Word2  Frequency\n",
       "0       turku       turku  17.314512\n",
       "1       lille       lille  17.075681\n",
       "2      aacute      aacute  17.010178\n",
       "3  louisville  louisville  16.999207\n",
       "4      gerais       minas  16.993550"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading surface cooccurrences into a pandas dataframe.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/genetics_surface_cooccurrences.csv\", encoding='UTF-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "30df88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ab67fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of unique words in our word pairs. \n",
    "unique_words = list(set(df[\"Word1\"].unique().tolist() \n",
    "                        + df[\"Word2\"].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "4f27afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this list of keywords an average embedding can be found to which each word can be compared.\n",
    "keywords = [\"genetics\",\"evolution\",\"ecology\",\"organism\",\"species\",\"population\",\"cells\",\"DNA\", \"RNA\",\"protein\", \n",
    "         \"enzymes\", \"metabolism\", \"reproduction\", \"adaptation\", \"biodiversity\", \"photosynthesis\",\"respiration\", \n",
    "         \"homeostasis\",\"osmosis\",\"diffusion\",\"mitosis\",\"meiosis\",\"chromosomes\",\"mutation\",\"gene\",\"heredity\",\n",
    "         \"natural\", \"selection\",\"phylogeny\",\"taxonomy\",\"anatomy\",\"physiology\",\"biochemistry\",\"molecular\",\n",
    "         \"microbiology\",\"virology\",\"immunology\",\"neuroscience\",\"developmental\",\"plant\",\"zoology\", \"antibody\",\n",
    "         \"chemistry\", \"virus\", \"bacteria\", \"organ\", \"mitochondria\", \"chloroplast\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filter_words = stop_words.union(\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"so\", \"as\", \"if\", \"then\",\n",
    "    \"in\", \"on\", \"at\", \"with\", \"without\", \"for\", \"by\", \"about\", \"of\", \"from\", \"to\",\n",
    "    \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "    \"do\", \"does\", \"did\", \"doing\", \"will\", \"would\", \"can\", \"could\", \"shall\", \"should\", \"may\", \"might\", \"must\",\n",
    "    \"this\", \"that\", \"these\", \"those\", \"there\", \"where\", \"when\", \"how\", \"why\", \"which\", \"who\", \"whom\", \"whose\",\n",
    "    \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"my\", \"your\", \"his\", \"its\", \"our\", \"their\",\n",
    "    \"mine\", \"yours\", \"hers\", \"ours\", \"theirs\",\n",
    "    \"all\", \"any\", \"some\", \"many\", \"several\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",\n",
    "    \"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\",\n",
    "    \"more\", \"less\", \"most\", \"least\", \"own\", \"other\", \"another\", \"each\", \"every\", \"much\", \"such\", \"few\", \"both\", \"either\", \"neither\",\n",
    "    \"thing\", \"place\", \"time\", \"person\", \"people\", \"man\", \"woman\", \"child\", \"year\", \"day\", \"month\", \"week\", \"hour\", \"minute\", \"second\")\n",
    "\n",
    "academic_words = [\n",
    "    \"analysis\", \"approach\", \"area\", \"assessment\", \"assume\", \"authority\", \"available\", \"benefit\", \"concept\", \"consistent\", \"constitutional\", \"context\", \"contract\", \"create\",\n",
    "    \"data\", \"definition\", \"derived\", \"distribution\", \"economic\", \"environment\", \"established\",\n",
    "    \"achieve\", \"acquisition\", \"administration\", \"affect\", \"appropriate\", \"aspects\", \"assistance\", \"categories\", \"chapter\", \"commission\", \"community\", \"complex\", \"computer\", \"conclusion\", \"conduct\", \"consequences\", \"construction\", \"consumer\", \"credit\",\n",
    "    \"cultural\",\n",
    "    \"estimate\", \"evidence\", \"export\",\n",
    "    \"factors\", \"financial\", \"formula\", \"function\", \"identified\", \"income\", \"indicate\", \"individual\", \"interpretation\", \"involved\", \"issues\",\n",
    "    \"labour\", \"legal\", \"legislation\", \"major\", \"method\", \"occur\", \"percent\",\n",
    "    \"design\", \"distinction\", \"elements\", \"equation\", \"evaluation\", \"features\", \"final\",\n",
    "    \"focus\",\n",
    "    \"impact\",\n",
    "    \"injury\", \"institute\", \"investment\", \"items\",\n",
    "    \"journal\", \"maintenance\", \"normal\", \"obtained\", \"participation\", \"perceived\", \"positive\",\n",
    "    \"period\", \"policy\", \"principle\", \"procedure\", \"process\", \"required\", \"research\", \"response\", \"role\", \"section\", \"sector\", \"significant\", \"similar\", \"source\", \"specific\", \"structure\", \"theory\", \"variables\",\n",
    "    \"potential\", \"previous\", \"primary\", \"purchase\", \"range\", \"region\", \"regulations\", \"relevant\", \"resident\", \"resources\", \"restricted\", \"security\", \"sought\", \"select\",\n",
    "    \"site\", \"strategies\", \"survey\", \"text\", \"traditional\", \"transfer\"\n",
    "]\n",
    "\n",
    "keyword_inputs = [tokenizer(word, return_tensors=\"pt\") for word in keywords]\n",
    "filter_word_inputs = [tokenizer(word, return_tensors=\"pt\") for word in filter_words]\n",
    "\n",
    "# Tokenizing the set of unique words.\n",
    "unique_word_inputs = tokenizer(unique_words, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "9909f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "# Get embeddings.\n",
    "with torch.no_grad():\n",
    "    keyword_outputs = [model(**input) for input in keyword_inputs]\n",
    "    keyword_embeddings = [output.last_hidden_state.mean(dim=1).numpy() for output in keyword_outputs]\n",
    "    \n",
    "    filter_word_outputs = [model(**input) for input in filter_word_inputs]\n",
    "    filter_word_embeddings = [output.last_hidden_state.mean(dim=1).numpy() for output in filter_word_outputs]\n",
    "    \n",
    "    unique_word_outputs = model(**unique_word_inputs)\n",
    "    unique_word_embeddings = unique_word_outputs.last_hidden_state..mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "d518532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarities between the keywords embedding and the list of unique words.\n",
    "keyword_embeddings = np.vstack(keyword_embeddings)\n",
    "mean_keyword_embedding = np.mean(keyword_embeddings, axis=0)\n",
    "mean_keyword_embedding = mean_keyword_embedding.reshape(1, -1)\n",
    "\n",
    "keyword_similarities = cosine_similarity(mean_keyword_embedding, unique_word_embeddings)[0]\n",
    "keyword_similarity_dict = {word: similarity for word, similarity in zip(unique_words, similarities)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "630edab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_to_filter(filter_word_embeddings=filter_word_embeddings,\n",
    "               threshold=0.875, inverse=False):\n",
    "    words_to_filter = set()\n",
    "    for i, filter_word_embedding in enumerate(filter_word_embeddings):\n",
    "        filter_word_embedding = filter_word_embedding.reshape(1, -1)\n",
    "        similarities = cosine_similarity(filter_word_embedding, unique_word_embeddings)[0]\n",
    "        # Go through the dataframe and remove words over the similarity threshold.\n",
    "        for i in range(len(similarities)):\n",
    "            if similarities[i] >= threshold:\n",
    "                words_to_filter.add(unique_words[i])\n",
    "    return words_to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "899b4e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'environmental', 'practical', 'grass', 'seven', 'guard', 'campus', 'survive', 'simultaneous', 'throughout', 'bath', 'consideration', 'sexual', 'tours', 'promotes', 'developed', 'treat', 'cook', 'behind', 'breeding', 'military', 'tertiary', 'addition', 'scholar', 'characterization', 'dominant', 'dei', 'persistent', 'juvenile', 'reduces', 'ecological', 'address', 'ornamental', 'southwestern', 'evaluate', 'demographic', 'authority', 'still', 'safe', 'lake', 'promote', 'della', 'less', 'newly', 'linear', 'nine', 'apply', 'multi', 'undertake', 'various', 'pharmaceutical', 'require', 'ethnic', 'obtain', 'muscular', 'sts', 'globe', 'relatively', 'trigger', 'pivotal', 'numerous', 'induce', 'con', 'fork', 'broadly', 'princess', 'harbour', 'get', 'alleviate', 'geographic', 'achieve', 'healthcare', 'represent', 'using', 'minimal', 'short', 'exists', 'couple', 'regulate', 'operate', 'medical', 'civil', 'impact', 'whose', 'promising', 'based', 'indigenous', 'create', 'interact', 'prince', 'accompany', 'bacterial', 'ten', 'studied', 'sampling', 'que', 'meanwhile', 'widespread', 'operating', 'without', 'scar', 'pan', 'need', 'circulating', 'academy', 'maintain', 'advent', 'psychiatric', 'botanical', 'par', 'bureau', 'necessarily', 'invest', 'another', 'therapeutic', 'agriculture', 'stimulate', 'certain', 'respect', 'suggest', 'define', 'fully', 'council', 'sensitive', 'harmful', 'admit', 'artificial', 'people', 'altogether', 'affiliation', 'additional', 'royalty', 'shareholder', 'poorly', 'surgical', 'dental', 'hunter', 'could', 'responsible', 'mostly', 'limited', 'day', 'publishers', 'livestock', 'commercial', 'time', 'farm', 'emerging', 'fisheries', 'detect', 'majority', 'important', 'rice', 'municipal', 'make', 'chair', 'comprehensive', 'fifth', 'player', 'genetically', 'serve', 'six', 'definitive', 'beneficial', 'temporal', 'marine', 'full', 'consumer', 'compared', 'related', 'four', 'meet', 'raise', 'affected', 'largely', 'emphasize', 'examine', 'opposite', 'manifest', 'yet', 'reef', 'well', 'stool', 'existence', 'ninth', 'childhood', 'occurrence', 'contributed', 'overcome', 'confirm', 'acquire', 'remains', 'complete', 'fort', 'clinic', 'suffer', 'unrelated', 'stone', 'include', 'nature', 'rue', 'toll', 'exist', 'diagnostic', 'gut', 'timing', 'fan', 'minor', 'observed', 'lack', 'agricultural', 'similarly', 'federation', 'kingdom', 'particularly', 'educational', 'technological', 'three', 'prospective', 'habitat', 'geographical', 'ace', 'prevalent', 'among', 'driver', 'generate', 'second', 'third', 'temple', 'due', 'suite', 'otherwise', 'consist', 'different', 'harbor', 'nothing', 'extensive', 'precise', 'useful', 'collectively', 'fourth', 'chapel', 'sixth', 'assess', 'specify', 'altered', 'similar', 'western', 'extreme', 'occur', 'consulting', 'pending', 'army', 'despite', 'vulnerable', 'chance', 'manner', 'aquatic', 'would', 'king', 'relevant', 'vegetation', 'next', 'non', 'arts', 'respiratory', 'identical', 'way', 'corporation', 'eight', 'warrant', 'costa', 'national', 'administer', 'novel', 'whereas', 'duke', 'boulder', 'saint', 'excessive', 'behalf', 'primarily', 'others', 'underwent', 'reproductive', 'severe', 'conserved', 'applied', 'consecutive', 'memorial', 'accelerate', 'fundamental', 'two', 'undergone', 'mimic', 'assign', 'allied', 'combine', 'independently', 'northwestern', 'fund', 'infectious', 'tower', 'regard', 'across', 'approve', 'manage', 'associated', 'forestry', 'seizure', 'royal', 'observation', 'incorporate', 'jersey', 'wildlife', 'commonly', 'administrative', 'cure', 'child', 'reduce', 'knight', 'widely', 'disturbance', 'arise', 'molecular', 'involve', 'idea', 'crop', 'might', 'affiliated', 'receiver', 'emeritus', 'underlying', 'ongoing', 'equally', 'impaired'}\n"
     ]
    }
   ],
   "source": [
    "filter_set = get_words_to_filter(threshold=0.862)\n",
    "print(filter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "36af1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df, column_name, filter_set, inverse=False):\n",
    "    if inverse == True:\n",
    "        df = df[df[column_name].isin(filter_set)]\n",
    "    else:\n",
    "        df = df[~df[column_name].isin(filter_set)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "3acf20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_dataframe(df, 'Word1', filter_set, inverse=False)\n",
    "df = filter_dataframe(df, 'Word2', filter_set, inverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e5770c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/genetics_filtered_surface_cooccurrences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543ae6f",
   "metadata": {},
   "source": [
    "## *(Extra experimental stuff below)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2d8073d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_similarity_df = filter_dataframe(word_similarity_df, 'Word', filter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f76e7b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>biology</td>\n",
       "      <td>0.906111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>nature</td>\n",
       "      <td>0.905792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>fisheries</td>\n",
       "      <td>0.904539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>insects</td>\n",
       "      <td>0.903834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>timing</td>\n",
       "      <td>0.903503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Similarity\n",
       "4071    biology    0.906111\n",
       "1579     nature    0.905792\n",
       "5038  fisheries    0.904539\n",
       "1368    insects    0.903834\n",
       "562      timing    0.903503"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab47c1",
   "metadata": {},
   "source": [
    "The similarities list corresponds to the index of a word in the ```unique_word_embeddings``` list, and therefore also the ```unique_words``` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "39c04ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_similarity_df = pd.DataFrame({\"Word\": unique_words, \n",
    "#                                    \"Similarity\": similarities})\n",
    "\n",
    "word_similarity_df = word_similarity_df.sort_values(by=\"Similarity\", ascending=False)\n",
    "word_similarity_df.to_csv(\"data/sorted_word_similarities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "fc008f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>biology</td>\n",
       "      <td>0.906111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>nature</td>\n",
       "      <td>0.905792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>fisheries</td>\n",
       "      <td>0.904539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>insects</td>\n",
       "      <td>0.903834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>timing</td>\n",
       "      <td>0.903503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Similarity\n",
       "4071    biology    0.906111\n",
       "1579     nature    0.905792\n",
       "5038  fisheries    0.904539\n",
       "1368    insects    0.903834\n",
       "562      timing    0.903503"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "43807312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "k = 15\n",
    "kmeans = KMeans(n_clusters=k, random_state=101, n_init=10)\n",
    "kmeans.fit(unique_word_embeddings)\n",
    "word_cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "546d3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clusters_df = pd.DataFrame({\"Word\": unique_words, \"Cluster\":word_cluster_labels, \"Embedding\": unique_word_embeddings.tolist()})\n",
    "word_clusters_df = word_clusters_df.sort_values(by=\"Cluster\", ascending=True)\n",
    "word_clusters_df.to_csv(\"data/word_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b7f24d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>biologia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>auxin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>randomized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>assay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>oregon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Cluster\n",
       "1892    biologia        0\n",
       "1229       auxin        0\n",
       "5374  randomized        0\n",
       "1242       assay        0\n",
       "451       oregon        0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_clusters_df = word_clusters_df[[\"Word\", \"Cluster\"]]\n",
    "word_clusters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b2c8ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Word  Cluster\n",
      "2470         tag        5\n",
      "4076   insurance        5\n",
      "655    fertility        5\n",
      "631         fate        5\n",
      "4896       order        5\n",
      "...          ...      ...\n",
      "448         stem        5\n",
      "4858  attractive        5\n",
      "4319     network        5\n",
      "446    machinery        5\n",
      "4782       panel        5\n",
      "\n",
      "[619 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "desired_cluster = 5\n",
    "cluster_rows = word_clusters_df.query(f\"Cluster == {desired_cluster}\")\n",
    "print(cluster_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "373b1d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aacute</td>\n",
       "      <td>aacute</td>\n",
       "      <td>17.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gerais</td>\n",
       "      <td>minas</td>\n",
       "      <td>16.993550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aires</td>\n",
       "      <td>buenos</td>\n",
       "      <td>16.988152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>buenos</td>\n",
       "      <td>aires</td>\n",
       "      <td>16.977012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>compostela</td>\n",
       "      <td>santiago</td>\n",
       "      <td>16.884648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word1     Word2  Frequency\n",
       "2      aacute    aacute  17.010178\n",
       "4      gerais     minas  16.993550\n",
       "5       aires    buenos  16.988152\n",
       "6      buenos     aires  16.977012\n",
       "9  compostela  santiago  16.884648"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ad8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

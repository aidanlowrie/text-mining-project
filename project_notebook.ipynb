{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Notebook**\n",
    "\n",
    "This notebook documents the steps taken to produce the data - a series of relevant *subject, relationship, object* triplets - that will be visualised in a network in future steps. The process has been broken down as follows:\n",
    "\n",
    "0. **Package Installation**.\n",
    "\n",
    "1. **Downloading the Data** using E-Utilities and Biopython.\n",
    "\n",
    "2. **Relationship Extraction** using REBEL.\n",
    "\n",
    "3. **Key-Word Extraction** using KeyBERT and BioBERT.\n",
    "\n",
    "4. **Lemmatization and Entity Resolution** using Stanza.\n",
    "\n",
    "5. **Visualisation**.\n",
    "\n",
    "6. **Evaluation** using PyKEEN.\n",
    "\n",
    "7. Other Attempts at Entity Resolution. (Extra)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Package Installation.**\n",
    "\n",
    "This code lists the installations required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install keybert\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install Levenshtein\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install BioPython\n",
    "!{sys.executable} -m pip install pykeen\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Downloading the Data using E-Utilities and Biopython.**\n",
    "To download the data, we will make use of E-Utilities (NCBI Entrez Programming Utilities), a set of tools designed to facilitate the process of downloading large sets of bioinformatic data.\n",
    "\n",
    "A general introduction to the E-Utilities:\n",
    "- https://www.ncbi.nlm.nih.gov/books/NBK25497/\n",
    "- *'A set of nine server-side programs that provide a stable interface into the Entrez query and database system at the NCBI'*.\n",
    "- Uses a fixed URL syntax that translates a standard set of input parameters into the values necessary for various NCBI software components to search for and retrieve the requested data.\n",
    "- The E-utilities are therefore the structured interface to the Entrez system, which currently includes 38 databases covering a variety of biomedical data, including nucleotide and protein sequences, gene records, three-dimensional molecular structures, and the biomedical literature.\n",
    "- To access data, a piece of software posts an E-utility URL to NCBI, then retrieves the results of this and processes the data.\n",
    "- It can use any computer languages that can send a URL to the E-utilities server and interpret the XML response (i.e. Python, Perl, Java, C++).\n",
    "- NCBI requests that users limit requests to no more than 3 per second.\n",
    "\n",
    "A combination of *ESearch* and *EFetch* can be used to find and retrieve the relevant data.\n",
    "\n",
    "In order to make use of the tools, a program must post an 'E-Utility URL' to NCBI. ```BioPython``` is a library that provides a tool called Entrez to send these URLs using Python. \n",
    "\n",
    "Below, it is used to fetch all article abstracts related to the term *'biology'*. It then saves the data to one text file, 'bio_corpus.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Setting email.\n",
    "Entrez.email = 'aidanlowrie@example.com'\n",
    "\n",
    "# Setting search word.\n",
    "search_word = 'biology'\n",
    "\n",
    "# Search for PubMed articles related to the search word 'biology', returning up to 100,000 results. \n",
    "search_handle = Entrez.esearch(db='pubmed', term=search_word, retmax=100000)\n",
    "record = Entrez.read(search_handle)\n",
    "search_handle.close()\n",
    "\n",
    "# A list of uids is necessary for fetching the actual abstracts.\n",
    "uids = record['IdList']\n",
    "\n",
    "# Fetch the abstracts in XML form, so that the actual abstract may be extracted.\n",
    "fetch_handle = Entrez.efetch(db=\"pubmed\", id=','.join(uids), \n",
    "                             rettype=\"abstract\", retmode=\"xml\") # Return data in XML form.\n",
    "data = fetch_handle.read()\n",
    "fetch_handle.close()\n",
    "\n",
    "# Parsing the XML.\n",
    "root = ET.fromstring(data)\n",
    "# Extracting the abstracts themselves from the returned data.\n",
    "abstracts = root.findall(\".//AbstractText\")\n",
    "\n",
    "# Write the abstracts to a file.\n",
    "with open(\"data/bio_corpus.txt\", \"w\") as file:\n",
    "    for abstract in abstracts:\n",
    "        file.write(str(abstract.text) + \"\\n\\n\") # Two newlines are added between abstracts, for clarity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Relationship Extraction using REBEL**\n",
    "Relationship extraction involves finding triples - *subject (head)*, *relationship (type)* and *object (tail)* - in a corpus. While this can be achieved through a variety of machine learning techniques including pattern matching and supervised machine learning, we have chosen to use REBEL (Relationship Extraction By End-to-end Language generation).\n",
    "\n",
    "[REBEL](https://aclanthology.org/2021.findings-emnlp.204.pdf) is an open source relationship extraction seq2seq model released in 2021. We have chosen to use it due to its state-of-the-art performance and accessibility.\n",
    "\n",
    "Before being passed into the model, we used ```NLTK``` to break the corpus into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing nltk resources.\n",
    "import nltk\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Opening corpus.\n",
    "with open(\"data/bio_corpus.txt\", \"r\") as file:\n",
    "    corpus_text = file.read()\n",
    "\n",
    "corpus_sentences = sent_tokenize(corpus_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```extract_triplets``` function was lifted directly from the [huggingface REBEL docs](https://huggingface.co/Babelscape/rebel-large). It parses the text generated by REBEL into a list of triplets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse REBEL output into a list of triplets. \n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loads the REBEL model and writes the extracted triplets to a file. (This had to be carried out in batches over several nights, which is why the code was adapted to include a ```start_line```.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Loading the model.\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "# Establishing a start line.\n",
    "start_line = 0\n",
    "\n",
    "# The model can only handle tokens of max length 1024 tokens. Those exceeding this capacity aren't considered in the dataset. (This is very rare, but the check is necessary.)\n",
    "max_token_length = 1024\n",
    "\n",
    "# Opening a csv file for triplet storage.\n",
    "with open(\"data/triplets_batch4.csv\", \"w\") as file:\n",
    "    # Field names.\n",
    "    field_names = ['head', 'type', 'tail']\n",
    "    writer = csv.DictWriter(file, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    for i, sentence in enumerate(corpus_sentences):\n",
    "        if i > start_line and len(triplet_extractor.tokenizer.encode(sentence)) <= max_token_length:\n",
    "            extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "            triplets = extract_triplets(extracted_text[0])\n",
    "            for triplet in triplets:\n",
    "                writer.writerow(triplet)    \n",
    "\n",
    "# Merge triples into a single file. \n",
    "def csv_union(csv_path_list, output_csv_path):\n",
    "    dfs = []\n",
    "    for csv in csv_path_list:\n",
    "        dfs.append(pd.read_csv(csv))\n",
    "    df_union = pd.concat(dfs)\n",
    "    df_union.to_csv(output_csv_path, index=False)\n",
    "\n",
    "csv_union(csv_path_list=['data/triplets_batch1.csv', 'data/triplets_batch2.csv', 'data/triplets_batch3.csv', 'data/triplets_batch4.csv'], output_csv_path='data/triplets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. KeyWord Extraction using KeyBERT**\n",
    "\n",
    "Originally, we tried to extract keywords by getting cosine similarity scores of terms against an average 'keyword embedding', which was generated by averaging the embeddings of a range of biology-related terms. This proved to be ineffective. We later found out about ```KeyBERT```, a library designed to facilitate the process of keyword extraction. \n",
    "\n",
    "KeyBERT works similarly, but generates an average '**abstract embedding**' for each abstract, rather than relying on a curated 'keyword embedding'. Potential keywords are compared to this embedding by **cosine similarity**, just like in the original method. It automatically generates 'keyword' n-grams of any specified length.\n",
    "\n",
    "In the code, KeyBERT is used on top of ```BioBERT``` (a version of BERT pre-trained on a biological corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Loading the corpus.\n",
    "with open('data/bio_corpus.txt', 'r') as file:\n",
    "    abstracts = file.readlines()\n",
    "\n",
    "# Loading the BioBERT model.\n",
    "model_name = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "biobert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Loading the KeyBERT model, running on BioBERT \n",
    "kw_model = KeyBERT(model=biobert_model)\n",
    "\n",
    "# Loading triplets dataframe.\n",
    "df = pd.read_csv('data/triplets.csv', names=['head', 'type', 'tail'])\n",
    "# Remove null values.\n",
    "df = df[df['head'].notna() & df['tail'].notna()]\n",
    "\n",
    "# Creating a list of stopwords from NLTK.\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# Extracting keywords and write them to a CSV.\n",
    "keywords = set()\n",
    "with open(\"data/keywords.csv\", \"w\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Keyword'])\n",
    "    writer.writeheader()   \n",
    "    for abstract in abstracts:\n",
    "        # Extracting unigrams.\n",
    "        new_keyword_list = set([keyword for keyword, _ in kw_model.extract_keywords(abstract, keyphrase_ngram_range=(1, 1), stop_words=stop_words)])\n",
    "        novel_keywords = new_keyword_list - keywords\n",
    "        keywords.update(novel_keywords)\n",
    "        for novel_keyword in novel_keywords:\n",
    "            writer.writerow({'Keyword': novel_keyword})\n",
    "        # Extracting bigrams.\n",
    "        new_keyword_list = set([keyword for keyword, _ in kw_model.extract_keywords(abstract, keyphrase_ngram_range=(2, 2), stop_words=stop_words)])\n",
    "        novel_keywords = new_keyword_list - keywords\n",
    "        keywords.update(novel_keywords)\n",
    "        for novel_keyword in novel_keywords:\n",
    "            writer.writerow({'Keyword': novel_keyword})\n",
    "        # Extracting trigrams.\n",
    "        new_keyword_list = set([keyword for keyword, _ in kw_model.extract_keywords(abstract, keyphrase_ngram_range=(3, 3), stop_words=stop_words)])\n",
    "        novel_keywords = new_keyword_list - keywords\n",
    "        keywords.update(novel_keywords)\n",
    "        for novel_keyword in novel_keywords:\n",
    "            writer.writerow({'Keyword': novel_keyword})\n",
    "\n",
    "# Filter the dataframe based on a list of keywords.\n",
    "def filter_dataframe(df, relevant_column_names, filter_set):\n",
    "    for column_name in relevant_column_names:\n",
    "        # Split hyphenated words.\n",
    "        df[column_name] = df[column_name].str.replace('-', ' ')\n",
    "        # Remove words that are in the filter set.\n",
    "        df = df[~df[column_name].isin(filter_set)]\n",
    "    return df\n",
    "\n",
    "keyword_data = pd.read_csv('data/keywords.csv')\n",
    "keywords = keyword_data['Keyword'].tolist()\n",
    "filtered_df = filter_dataframe(df=df, relevant_column_names=['head', 'tail'], filter_set=keywords)\n",
    "filtered_df.to_csv('data/filtered_triplets.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Lemmatization and Entity Resolution using Stanza.**\n",
    "\n",
    "The library ```stanza``` was chosen for the task of lemmatization due to its biomedical lemmatization tools, trained on biomedical text. As a result, it was less likely to incorrectly classify words. Only unigrams were chosen to be lemmatized and merged as lemmatized bigrams / trigrams might be more difficult for humans to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "# Loading the data.\n",
    "df = pd.read_csv('data/filtered_triplets.csv')\n",
    "\n",
    "# Downloading the lemmatizer.\n",
    "stanza.download('en', package='craft')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma', package='craft')\n",
    "\n",
    "# Lemmatize a given text.\n",
    "def lemmatize_text(text):\n",
    "    if ' ' not in text:\n",
    "        doc = nlp(text)\n",
    "        lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "        return lemmas[0] if lemmas else text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Lemmatizing the head and tail columns.\n",
    "df['head'] = df['head'].apply(lemmatize_text)\n",
    "df['tail'] = df['tail'].apply(lemmatize_text)\n",
    "\n",
    "# Creating new 'pair' column. This will be how we determine the most common relationships between pairs. \n",
    "df['pair'] = df['head'] + ',' + df['tail']\n",
    "\n",
    "# Grouping by pair and type, counting frequency.\n",
    "df_grouped = df.groupby(['pair', 'type', 'head', 'tail']).size().reset_index(name='counts')\n",
    "\n",
    "# Sorting by pair and counts, and dropping all but one instance of the most common (first). \n",
    "df_most_common = df_grouped.sort_values(['pair', 'counts'], ascending=False).drop_duplicates(subset='pair').sort_index()\n",
    "\n",
    "# Dropping unnecessary 'pair' and 'counts' columns.\n",
    "df_most_common = df_most_common.drop(columns=['pair', 'counts'])\n",
    "\n",
    "# Reordering.\n",
    "df_most_common = df_most_common[['head', 'type', 'tail']]\n",
    "\n",
    "# Saving to CSV.\n",
    "df_most_common.to_csv('data/filtered_lemmatized_triplets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_most_common.to_csv('data/filtered_lemmatized_triplets.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Visualisation.**\n",
    "\n",
    "Visualisation code can be found in this notebook (Aidan) and a separate one (Boray), using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Read the CSV file into a pandas dataframe.\n",
    "df = pd.read_csv(\"data/filtered_lemmatized_triplets.csv\")\n",
    "\n",
    "# Create NetworkX DiGraph from dataframe.\n",
    "df.rename(columns={'type':'relation'}, inplace=True)\n",
    "G = nx.from_pandas_edgelist(df, source='head', target='tail', edge_attr='relation', create_using=nx.DiGraph())\n",
    "\n",
    "# Assign edge labels.\n",
    "for u, v, data in G.edges(data=True):\n",
    "    data['label'] = data['relation']\n",
    "\n",
    "nx.write_gexf(G, \"big_triplet_network.gexf\")\n",
    "\n",
    "import random\n",
    "\n",
    "def filter_edges_by_relation(G, relation_type):\n",
    "    H = G.copy()\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if data['relation'] != relation_type:\n",
    "            H.remove_edge(u, v)\n",
    "    return H\n",
    "\n",
    "def prune_graph(G, target_node, radius):\n",
    "    H = G.copy()\n",
    "\n",
    "    # Create an ego graph from the target node.\n",
    "    H = nx.ego_graph(G, target_node, radius=radius, center=True, undirected=False)\n",
    "    \n",
    "    # Iterate over all pairs of nodes.\n",
    "    for u, v in list(H.edges):\n",
    "        # If there are edges in both directions...\n",
    "        if H.has_edge(u, v) and H.has_edge(v, u):\n",
    "            # Compute shortest paths *from* the target node.\n",
    "            path_u = nx.shortest_path_length(H, target_node, u)\n",
    "            path_v = nx.shortest_path_length(H, target_node, v)\n",
    "            # Remove the edge that is farther from the target_node.\n",
    "            if path_u < path_v:\n",
    "                H.remove_edge(v, u)\n",
    "            elif path_u > path_v:\n",
    "                H.remove_edge(u, v)\n",
    "            else:\n",
    "                # If there's a tie, randomly remove one of the two edges.\n",
    "                if random.choice([True, False]):\n",
    "                    H.remove_edge(u, v)\n",
    "                else:\n",
    "                    H.remove_edge(v, u)\n",
    "    return H\n",
    "\n",
    "# Create subgraph for a particular target node.\n",
    "def generate_subgraph(G, target_node, radius, relation_type=None):\n",
    "    H = G.copy()\n",
    "    H = prune_graph(H, target_node=target_node, radius=radius)\n",
    "    if relation_type != None:\n",
    "        H = filter_edges_by_relation(H, relation_type)\n",
    "    nx.write_gexf(H, str('graphs/' + target_node + '_' + str(radius) + '_' + str(relation_type) + '.gexf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below can be used to generate a subgraph of a target term in the knowledge graph (node), with only one edge allowed between terms (preferring outgoing edges from nodes closest to the target). Examples of graphs created by this method can be found in the 'data/graphs' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_subgraph(G=G, target_node='telomere', radius=1, relation_type=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Evaluation using PyKEEN.**\n",
    "\n",
    "PyKEEN is a library that can be used to create predictor models based on Knowledge Graph embeddings (such as the information we have created by the triplet generation process). Below, PyKEEN is used to generate a predictor model, and then - after 100 iterations of training - the model's performance is evaluated. By evaluating the performance of the model, we can infer information about the KG itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "\n",
    "# Loading data.\n",
    "df = pd.read_csv('drive/MyDrive/data/filtered_lemmatized_triplets.csv')\n",
    "df = df.astype(str)\n",
    "\n",
    "# Creating a TriplesFactory object to pass into the pipeline.\n",
    "triples_factory = TriplesFactory.from_labeled_triples(df.values)\n",
    "# Splitting object into training and testing data.\n",
    "training, testing = triples_factory.split([.8, .2])  # 80% training, 20% testing.\n",
    "\n",
    "# Training the model.\n",
    "pipeline_result = pipeline(\n",
    "    model='nTransE', \n",
    "    training=training,\n",
    "    testing=testing,\n",
    "    random_seed=101,\n",
    "    device='cuda', \n",
    "    training_kwargs=dict(num_epochs=100),\n",
    ")\n",
    "\n",
    "# Evaluating the model.\n",
    "results_df = pipeline_result.metric_results.to_df()\n",
    "# Saving the evaluation.\n",
    "results_df.to_csv('drive/MyDrive/data/metric_results_lemmatized_triplets_model.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the processes used to create and evaluate the KG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Other Attempts at Entity Resolution. (Extra)**\n",
    "\n",
    "#### **Using BioBERT Embeddings and Levenshtein Distance.**\n",
    "While lemmatization is useful for normalising text, we hoped that using BioBERT embeddings would allow for entity resolution in more ambiguous cases (such as 'Alzheimers' and 'Alzheimers Disease'.) We made several attempts to perform entity resolution using BioBERT embeddings, but this proved ineffective. Our final attempt, which considered a combination of BioBERT embeddings and Levenshtein (edit) distance, is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import Levenshtein\n",
    "\n",
    "# Get a list of each unique term in the df.\n",
    "unique_words = list(set(df[\"head\"].unique().tolist() \n",
    "                        + df[\"tail\"].unique().tolist()))\n",
    "\n",
    "# Tokenise.\n",
    "unique_words_tokenised = [tokenizer(word, return_tensors=\"pt\") for word in unique_words]\n",
    "\n",
    "# Feeding the tokenised words into the model to get a list of unique_word_embeddings.\n",
    "with torch.no_grad():\n",
    "    unique_word_outputs = [biobert_model(**tokens) for tokens in unique_words_tokenised]\n",
    "unique_word_embeddings = [output.last_hidden_state.mean(dim=1).numpy() for output in unique_word_outputs]\n",
    "\n",
    "# Creating a DataFrame with the words and their embeddings\n",
    "df_embeddings = pd.DataFrame({\n",
    "    'word': unique_words,\n",
    "    'embedding': unique_word_embeddings\n",
    "})\n",
    "\n",
    "def get_cosine_similarity(df_embeddings, word_1, word_2):\n",
    "    # Getting the embeddings for the two words\n",
    "    embedding1_df = df_embeddings[df_embeddings['word'] == word_1]['embedding']\n",
    "    embedding2_df = df_embeddings[df_embeddings['word'] == word_2]['embedding']\n",
    "    if embedding1_df.empty or embedding2_df.empty:\n",
    "        return 0\n",
    "    else:\n",
    "        embedding1 = embedding1_df.values[0]\n",
    "        embedding2 = embedding2_df.values[0]\n",
    "        # Computing and returning cosine similarity scores\n",
    "        return cosine_similarity(embedding1, embedding2)[0][0] # type: ignore\n",
    "\n",
    "def get_levenshtein_similarity(word_1, word_2):\n",
    "    return 1 - Levenshtein.distance(word_1, word_2) / max(len(word_1), len(word_2))\n",
    "\n",
    "# Merging similar words by rippling through the keyword list and comparing against others, then removing from list.\n",
    "def merge_similar_words(df, df_embeddings, cosine_threshold, levenshtein_threshold):\n",
    "    word_replacements = {}\n",
    "    unique_words = df_embeddings['word'].to_list()\n",
    "    for word_1 in unique_words:\n",
    "        unique_words.remove(word_1)\n",
    "        for word_2 in unique_words:\n",
    "            levenshtein_score = get_levenshtein_similarity(word_1=word_1, word_2=word_2)\n",
    "            cosine_score = get_cosine_similarity(df_embeddings=df_embeddings, word_1=word_1, word_2=word_2)\n",
    "            if  (cosine_score > cosine_threshold and levenshtein_score > levenshtein_threshold) or levenshtein_score > 0.85:\n",
    "                print('SUCCESS', '1', word_1, '2', word_2, 'cos', cosine_score, 'lev', levenshtein_score)\n",
    "                if word_1.lower() == word_1 or word_2.lower() == word_2:\n",
    "                    word_1 = word_1.lower()\n",
    "                    word_2 = word_2.lower()\n",
    "                if len(word_1) < len(word_2):\n",
    "                    winner = word_1\n",
    "                elif len(word_2) < len(word_1):\n",
    "                    winner = word_2\n",
    "                else:\n",
    "                    winner = random.choice((word_1, word_2))\n",
    "                if winner == word_1:\n",
    "                    loser = word_2\n",
    "                else:\n",
    "                    loser = word_1\n",
    "                word_replacements[loser] = winner\n",
    "    df_replaced = df.replace(word_replacements)\n",
    "    return df_replaced\n",
    "\n",
    "df_replaced = merge_similar_words(df=df, df_embeddings=df_embeddings, cosine_threshold=0.97, levenshtein_threshold=0.6)\n",
    "df_replaced.to_csv('data/final_triplets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issue was that some words would be considered very semantically similar to just about everything. To limit this, the threshold was increased and the levenshtein scores were introduced. But problems remained. Terms like *RNA virus* and *DNA virus* would be merged by the algorithm, despite clearly referring to different entities. Another approach was needed.\n",
    "\n",
    "#### **Finetuning an Entity Resolution Model.**\n",
    "Next, we tried to finetune a BioBERT model for the task of entity resolution. I still think that this is a promising approach, but the dataset for it just doesn't exist yet. I tried to make one myself, but without sufficient time or resources, such a task has proved to be impossible. With a large enough dataset, I anticipate that this could have been a successful solution to the Entity Resolution task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "\n",
    "# Loading BioBERT model and tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "\n",
    "# Preparing the dataset.\n",
    "class MergingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_a, text_b, label = self.data[idx]\n",
    "        inputs = self.tokenizer.encode_plus(text_a, text_b,\n",
    "                                            padding='max_length',\n",
    "                                            max_length=512,\n",
    "                                            truncation=True,\n",
    "                                            return_tensors='pt')\n",
    "        inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Loading the data.\n",
    "data_df = pd.read_csv('drive/MyDrive/data/classifier_training_data.csv', usecols=[0, 1, 2], header=None, names=['Text_A', 'Text_B', 'Label'])\n",
    "data_df = data_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "data = [(row['Text_A'], row['Text_B'], int(row['Label'])) for index, row in data_df.iterrows()]\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.05, random_state=10101)\n",
    "\n",
    "train_dataset = MergingDataset(train_data, tokenizer)\n",
    "val_dataset = MergingDataset(val_data, tokenizer)\n",
    "\n",
    "# Fine-tuning the model.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct_predictions = predictions == batch['labels']\n",
    "            total_eval_accuracy += correct_predictions.sum().item()\n",
    "\n",
    "    average_val_accuracy = total_eval_accuracy / len(val_dataset)\n",
    "    average_val_loss = total_eval_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Validation Accuracy for epoch {epoch+1}: {average_val_accuracy}\")\n",
    "    print(f\"Validation Loss for epoch {epoch + 1}: {average_val_loss}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "model.save_pretrained(\"drive/MyDrive/data/er_model_2\")\n",
    "\n",
    "# Loading BioBERT model and tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertForSequenceClassification.from_pretrained('drive/MyDrive/data/er_model_2')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Loading triplets dataframe.\n",
    "df = pd.read_csv('drive/MyDrive/data/filtered_triplets.csv', names=['head', 'type', 'tail'])\n",
    "df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "\n",
    "# Remove null values.\n",
    "df = df[df['head'].notna() & df['tail'].notna()]\n",
    "\n",
    "# Get a list of each unique term in the df.\n",
    "unique_terms = list(set(df[\"head\"].unique().tolist() \n",
    "                        + df[\"tail\"].unique().tolist()))\n",
    "\n",
    "to_merge = set()\n",
    "\n",
    "for term1, term2 in combinations(unique_terms, 2):\n",
    "    inputs = tokenizer.encode_plus(term1, term2, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        if prediction == 1:  # If the model predicts that the terms should be merged...\n",
    "            # Choose the shorter term to keep.\n",
    "            shorter_term = term1 if len(term1) <= len(term2) else term2\n",
    "            print(f'Merging {term1} and {term2}, keeping {shorter_term}')\n",
    "            to_merge.add(shorter_term)\n",
    "\n",
    "# `to_merge` now contains all terms that should be merged according to the model.\n",
    "print(f\"Terms to merge: {to_merge}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
